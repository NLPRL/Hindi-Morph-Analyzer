['loss', 'activation_1_loss', 'dense_2_loss', 'dense_3_loss', 'dense_4_loss', 'dense_5_loss', 'dense_6_loss', 'dense_7_loss', 'dense_8_loss', 'activation_1_acc', 'dense_2_acc', 'dense_3_acc', 'dense_4_acc', 'dense_5_acc', 'dense_6_acc', 'dense_7_acc', 'dense_8_acc'] = [17.624994389805398, 1.0694421564600998, 2.8581765618451116, 2.1029987881973784, 1.8667988971333318, 1.613826800138906, 1.2483490493810065, 2.5967357850263308, 4.268666334584152, 0.89593313040767131, 0.66242169045907362, 0.60359967135668069, 0.68249974324740681, 0.70481154359659037, 0.61091712026291467, 0.65289616925130944, 0.13387080209510116]

Activation_1_acc denotes the accuracy for the root word prediction of each word. The accuracy metric isn't reliable here though, I have put that here for now. 

Dense_2_acc denotes the accuracy for prediciting POS categories, i.e. 'n', 'psp', 'v, 'adj', etc. The acc for this is 66.24%

Dense_3_acc denotes the accuracy for gender prediction => 60.36%

Dense_4_acc denotes the word's number prediction, i.e. 'sg', 'pl' , etc. => 68.25%

Dense_5_acc  denotes score of person prediction => 70.48%

Dense_6_acc denotes the case prediction => 61.09%

Dense_7_acc denotes the Tense Aspect Modality prediction => 65.29%

Dense_8_acc denotes the chunkId prediction score => 13.39%  

NOTE: I have mentioned the labels for each of these categories in the file data_percentage.txt


############### Test time reports ################
Total test instances = 38,948
Simple BLSTM model with a context of 1 word back and forth predicts the root as well as the six features for all 38,948 instances in 105s, i.e. an average of 370.93 instances per second!
blstm + context1 + attention took 108s for same.
blstm + context2 = 147s (tam = 0.81)
blstm + context2 + att = 145s (tam = 0.72)
" " + context3 + att = 177s (tam = 0.79)
" " + cont3 = 10 epoch (tam = 0.81)
" " + cont4 = 10 ep 210s (tam= 0.68)
cont4 + att = 10 ep (tam=0.78)

##### No of training epochs for convergence #######
context1, simple BLSTM = 21 epochs
context1, with attention (Bahdanau) = 26
context2 = 12


############## after the correction on context ###############
blstm + context 2 + att = 157s (tam = 0.74)



# 1. char cnn with rnn
convergence = 8 epoch, test time = 70s 
Total params: 3,759,545
Trainable params: 3,759,545
Non-trainable params: 0 (tam = 0.82)

2. with global avg pool instead of max pool , same architecture
8 epoch, but val loss remains constant now unlike previous increasing trend
3,759,545 params => 68s (tam=0.83)

3 global avg pool with noise added
epochs 15, tam = 0.86
params = 2,961,461 => 71s (scale of 0.7 added)
